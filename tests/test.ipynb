{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import time\n",
    "import emoji\n",
    "import random\n",
    "import pandas\n",
    "import pymysql\n",
    "import logging\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "def get_headers(url, use='pc'):\n",
    "    pc_agents = [\n",
    "        \"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
    "        \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
    "        \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0);\",\n",
    "        \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)\",\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)\",\n",
    "        \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\n",
    "        \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11\",\n",
    "        \"Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)\",\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)\",\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)\",\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)\",\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)\",\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Avant Browser)\",\n",
    "        \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36\"\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64; rv:76.0) Gecko/20100101 Firefox/76.0\"\n",
    "    ]\n",
    "    phone_agents = [\n",
    "        \"Mozilla/5.0 (iPhone; U; CPU iPhone OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5\",\n",
    "        \"Mozilla/5.0 (iPod; U; CPU iPhone OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5\",\n",
    "        \"Mozilla/5.0 (iPad; U; CPU OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5\",\n",
    "        \"Mozilla/5.0 (Linux; U; Android 2.3.7; en-us; Nexus One Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1\",\n",
    "        \"MQQBrowser/26 Mozilla/5.0 (Linux; U; Android 2.3.7; zh-cn; MB200 Build/GRJ22; CyanogenMod-7) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1\",\n",
    "        \"Opera/9.80 (Android 2.3.4; Linux; Opera Mobi/build-1107180945; U; en-GB) Presto/2.8.149 Version/11.10\",\n",
    "        \"Mozilla/5.0 (Linux; U; Android 3.0; en-us; Xoom Build/HRI39) AppleWebKit/534.13 (KHTML, like Gecko) Version/4.0 Safari/534.13\",\n",
    "        \"Mozilla/5.0 (BlackBerry; U; BlackBerry 9800; en) AppleWebKit/534.1+ (KHTML, like Gecko) Version/6.0.0.337 Mobile Safari/534.1+\",\n",
    "        \"Mozilla/5.0 (hp-tablet; Linux; hpwOS/3.0.0; U; en-US) AppleWebKit/534.6 (KHTML, like Gecko) wOSBrowser/233.70 Safari/534.6 TouchPad/1.0\",\n",
    "        \"Mozilla/5.0 (SymbianOS/9.4; Series60/5.0 NokiaN97-1/20.0.019; Profile/MIDP-2.1 Configuration/CLDC-1.1) AppleWebKit/525 (KHTML, like Gecko) BrowserNG/7.1.18124\",\n",
    "        \"Mozilla/5.0 (compatible; MSIE 9.0; Windows Phone OS 7.5; Trident/5.0; IEMobile/9.0; HTC; Titan)\",\n",
    "        \"UCWEB7.0.2.37/28/999\",\n",
    "        \"NOKIA5700/ UCWEB7.0.2.37/28/999\",\n",
    "        \"Openwave/ UCWEB7.0.2.37/28/999\",\n",
    "        \"Mozilla/4.0 (compatible; MSIE 6.0; ) Opera/UCWEB7.0.2.37/28/999\"\n",
    "    ]\n",
    "    referer = lambda url: re.search(\n",
    "        \"^((http://)|(https://))?([a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?\\.)+[a-zA-Z]{2,6}(/)\", url).group()\n",
    "    if use == 'phone': # 随机获取一个headers\n",
    "        agent = random.choice(phone_agents)\n",
    "    else:\n",
    "        agent = random.choice(pc_agents)\n",
    "    headers = {\n",
    "        'User-Agent': agent,\n",
    "        'Referer': referer(url),\n",
    "        'DNT': \"1\",\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Accept-Language': 'zh-CN,zh;q=0.9,en-CN;q=0.8,en;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        # 'Accept-Encoding': 'gzip, deflate, br',\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "# logging.FileHandler(filename='豆瓣哈组标题和链接.log', encoding='utf-8')\n",
    "# logging.basicConfig(level=logging.INFO,\n",
    "#                     format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',\n",
    "#                     datefmt='%Y-%m-%d %A %H:%M:%S',\n",
    "#                     filename='C:\\\\Users\\\\Administrator\\\\Desktop\\\\豆瓣哈组标题和链接.log', # 日志保存路径\n",
    "#                     filemode='w')\n",
    "\n",
    "# 连接database\n",
    "conn = pymysql.connect(\n",
    "    host = '127.0.0.1',\n",
    "    user = 'root', \n",
    "    password = 'usbw',\n",
    "    database = 'douban',\n",
    "    charset = 'utf8mb4')\n",
    "# 得到一个可以执行SQL语句的光标对象\n",
    "cursor = conn.cursor()  # 执行完毕返回的结果集默认以元组显示                   \n",
    "\n",
    "topic_values = {'group': None, 'title': None, 'author': None, 'link': None, 'time': None, 'topic_id':None}\n",
    "\n",
    "\n",
    "def get(start_page=1, end_page=1):\n",
    "    while start_page <= end_page:\n",
    "        # logging.info('[get] 已进入小组第' + str(start_page) + '页')\n",
    "        print('[get] 已进入小组第' + str(start_page) + '页')\n",
    "        url = 'https://www.douban.com/group/638298/discussion?start=' + str((start_page - 1) * 25) # 哈哈哈哈哈哈哈哈哈哈哈小组\n",
    "        # print(url)\n",
    "        data = requests.get(url, headers=get_headers(url))\n",
    "        print('start_page:', start_page, 'resp:', data.ok)\n",
    "        data.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(data.text, 'html.parser')\n",
    "        links = soup.select('.title')\n",
    "        for link in links:\n",
    "            _link = str(link.select('a')[0])\n",
    "            if _link.find('title=\"', 0, len(_link)) > -1:\n",
    "\n",
    "                topic_values['group'] = '哈哈哈哈哈哈哈哈哈哈哈小组'\n",
    "                topic_values['title'] = link.select('a')[0]['title'] # 提取出话题标题\n",
    "                topic_values['link'] = link.select('a')[0]['href'] # 提取出话题链接\n",
    "                topic_values['topic_id'] = int(re.sub(\"\\D\", \"\", topic_values['link']))\n",
    "\n",
    "\n",
    "                sql = 'insert ignore into hazu_copy1(group_name,title,link,topic_id) values(\"%s\",\"%s\", \"%s\", %s);' %(topic_values['group'], topic_values['title'], topic_values['link'], topic_values['topic_id'])\n",
    "                cursor.execute(sql)\n",
    "                # 以字符串形式书写SQL语句R\n",
    "                # 拼接并执行sql语句\n",
    "                # cursor.executemany(sql, data)\n",
    "                # local_var = cursor.executemany(sql, data)\n",
    "                # print(local_var)\n",
    "                # sql = 'insert ignore into hazu(group,title,link,topic_id) values(%s,%s,%s,%s);' %(group, title, href, num) # insert ignore 表示，如果中已经存在相同的记录，则忽略当前新数据；\n",
    "                # print(sql)\n",
    "                # cursor.execute(sql)\n",
    "                # 以字符串形式书写SQL语句\n",
    "                # 获取发帖时间\n",
    "                # data = requests.get(href, headers=get_headers(href))\n",
    "                # data.encoding = 'utf-8'\n",
    "                # soup = BeautifulSoup(data.text, 'html.parser')\n",
    "                # timesource = soup.select('.color-green')[0].text\n",
    "                # _time = datetime.strptime(timesource, '%Y-%m-%d %H:%M:%S')\n",
    "                \n",
    "        conn.commit()\n",
    "        start_page += 1\n",
    "        time.sleep(5)\n",
    "        if start_page > end_page:\n",
    "            start_page = 1\n",
    "    # 关闭连接\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "def seve_excel():\n",
    "    # df_1 = pandas.DataFrame.from_dict({'发帖时间' : pandas.Categorical(topic_times),\n",
    "    #                 '作者' : pandas.Categorical(topic_author),\n",
    "    #                 '标题' : pandas.Categorical(topic_titles),\n",
    "    #                 '链接' : pandas.Categorical(topic_links),\n",
    "    #                 }, orient='index')\n",
    "    df_1 = pandas.DataFrame({'标题' : pandas.Categorical([1,2,3,4,5]),\n",
    "                '链接' : pandas.Categorical([1,2,3,4,5]),\n",
    "                })\n",
    "    print(df_1)\n",
    "    df_1.to_excel('豆瓣哈组数据.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "def mysql_test():\n",
    "    # 导入pymysql模块\n",
    "    import pymysql\n",
    "    # 连接database\n",
    "    conn = pymysql.connect(\n",
    "        host = '127.0.0.1',\n",
    "        user = 'root', \n",
    "        password = 'usbw',\n",
    "        database = 'douban',\n",
    "        charset = 'utf8mb4')\n",
    "    # 得到一个可以执行SQL语句的光标对象\n",
    "    cursor = conn.cursor()  # 执行完毕返回的结果集默认以元组显示\n",
    "    # 得到一个可以执行SQL语句并且将结果作为字典返回的游标\n",
    "    # 定义要执行的sql语句\n",
    "    print(type(cursor))\n",
    "\n",
    "    group11 = '哈哈哈哈哈哈哈哈哈哈哈小组'\n",
    "    title11 = '原创加水印｜很野的淘宝图片识别'\n",
    "    href11 = 'https://www.douban.com/group/topic/190342525/'\n",
    "    num11 = int(190342525)\n",
    "\n",
    "    sql = \"insert ignore into hazu(group_name,title,link,topic_id) values(%s,%s,%s,%s);\" %(group11, title11, href11, num11) # insert ignore 表示，如果中已经存在相同的记录，则忽略当前新数据；\n",
    "    sql_3 = 'insert ignore into hazu(group_name,title,link,topic_id) values(\"%s\",\"%s\", \"%s\", %s);' %(group11, title11, href11, num11)\n",
    "    var = cursor.execute(sql_3)\n",
    "    print(var)\n",
    "    conn.commit()  # 事务提交\n",
    "\n",
    "    \n",
    "    # cursor.execute(sql_1)\n",
    "    # sql = 'insert into test(link,title) values(%s,%s);'\n",
    "\n",
    "    # 以字符串形式书写SQL语句\n",
    "\n",
    "    # 拼接并执行sql语句\n",
    "    # cursor.executemany(sql, data)\n",
    "\n",
    "    # try:\n",
    "    #     cursor.executemany(sql_2, data)\n",
    "    # except Exception as e:\n",
    "    #     conn.rollback()  # 事务回滚\n",
    "    #     print('事务处理失败', e)\n",
    "    # else:\n",
    "    #     # 涉及写操作要注意提交\n",
    "    #     conn.commit()  # 事务提交\n",
    "    #     print('事务处理成功', cursor.rowcount)\n",
    "\n",
    "    # 更新\n",
    "    # sql = 'update test set title=\"2222\", author=\"小贱\" where link=\"01\";'\n",
    "    # 拼接并执行sql语句\n",
    "    # cursor.execute(sql)\n",
    "\n",
    "\n",
    "    # insert ignore into table_name(email,phone,user_id) values('test9@163.com','99999','9999'); # insert ignore 表示，如果中已经存在相同的记录，则忽略当前新数据；\n",
    "    # insert replace into table_name(email,phone,user_id) values('test9@163.com','99999','9999'); # insert replace 表示，如果中已经存在相同的记录，则忽略当前新数据；\n",
    "    \n",
    "    sql = 'SELECT MAX(id) FROM test;' # 获取test表下id字段最大的值\n",
    "    # sql = 'insert ignore into test(link, title, author) values(\"test9@163.com\", \"12345\", \"56789\");'\n",
    "    cursor.execute(sql)\n",
    "    data = cursor.fetchone()\n",
    "    print(data)\n",
    "    # 逻辑:一阶段用insert ignore直接存入全部需要存入的数据,二阶段进入链接获取图片发帖时间等信息后进行一个update操作.(二阶段只会建立在一阶段前提下进行,所以二阶段只需要进行更新数据即可.)\n",
    "\n",
    "    '''\n",
    "    SELECT MAX(Age) FROM Student {查询学生表中年级最大的}\n",
    "    SELECT MIN(Age) FROM Student {查询学生表中年级最小的}\n",
    "    SELECT AVG(Age) FROM Student {查询学生的平均年级}\n",
    "    SELECT COUNT(*) FROM Student {查询表中的总记录}\n",
    "    '''\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    # 涉及写操作要注意提交\n",
    "    # conn.commit()\n",
    "    \n",
    "    # 关闭连接\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get()\n",
    "    # mysql_test()"
   ]
  }
 ]
}